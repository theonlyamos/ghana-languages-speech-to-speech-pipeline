{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ghana Speech-to-Speech Pipeline\n",
    "\n",
    "## Unified Multilingual Model for Akan, Ewe, Ga, and Dagbani\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive notebook covers the complete pipeline for building a Speech-to-Speech (S2S) system for Ghanaian languages:\n",
    "\n",
    "1. **Part 0**: Setup & System Verification\n",
    "2. **Part 1**: Dataset Download & Organization\n",
    "3. **Part 2**: Data Processing & Preparation\n",
    "4. **Part 3**: Model Training (ASR, TTS, Translation)\n",
    "5. **Part 4**: Unified Pipeline & Inference\n",
    "6. **Part 5**: Evaluation & Benchmarking\n",
    "7. **Part 6**: Deployment & Serving\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "+------------------+     +------------------+     +------------------+\n",
    "|    THE EAR       |     |    THE BRAIN     |     |    THE MOUTH     |\n",
    "|    (ASR)         | --> |    (Translation) | --> |    (TTS)         |\n",
    "|    Meta MMS      |     |    NLLB-200      |     |    XTTS v2       |\n",
    "+------------------+     +------------------+     +------------------+\n",
    "     Speech             Text (Twi)         Text (English)        Speech\n",
    "```\n",
    "\n",
    "### Supported Languages\n",
    "- **Akan (Twi/Fante)** - aka\n",
    "- **Ewe** - ewe\n",
    "- **Ga** - gaa\n",
    "- **Dagbani** - dag\n",
    "- **English** - eng\n",
    "\n",
    "---\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- GPU: NVIDIA RTX 3090/4090 (24GB VRAM) recommended\n",
    "- RAM: 32GB+ recommended\n",
    "- Storage: 250GB+ free space for datasets\n",
    "\n",
    "**Estimated Time:**\n",
    "- Dataset Download: 2-6 hours (depending on connection)\n",
    "- ASR Training: 2-4 hours\n",
    "- TTS Training: 4-8 hours\n",
    "- Total: ~10-20 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 0: Setup & System Verification\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 GPU/CUDA Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # Set default device\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Storage Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Check available disk space\n",
    "total, used, free = shutil.disk_usage(\".\")\n",
    "\n",
    "print(\"STORAGE INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total: {total / 1024**3:.1f} GB\")\n",
    "print(f\"Used: {used / 1024**3:.1f} GB\")\n",
    "print(f\"Free: {free / 1024**3:.1f} GB\")\n",
    "\n",
    "# Minimum recommended space\n",
    "MIN_SPACE_GB = 100  # For sample mode\n",
    "FULL_SPACE_GB = 300  # For full datasets\n",
    "\n",
    "if free / 1024**3 < MIN_SPACE_GB:\n",
    "    print(f\"\\nWARNING: Less than {MIN_SPACE_GB}GB free. Consider using SAMPLE_MODE=True\")\n",
    "elif free / 1024**3 < FULL_SPACE_GB:\n",
    "    print(f\"\\nNOTE: Less than {FULL_SPACE_GB}GB free. Full dataset download may not fit.\")\n",
    "else:\n",
    "    print(\"\\nStorage: OK for full dataset download\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "# Uncomment and run if packages are not installed\n",
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install transformers accelerate datasets peft bitsandbytes\n",
    "# !pip install TTS  # Coqui TTS\n",
    "# !pip install librosa soundfile scipy\n",
    "# !pip install pandas openpyxl tqdm requests\n",
    "# !pip install jiwer evaluate sacrebleu  # For evaluation\n",
    "# !pip install gradio fastapi uvicorn python-multipart  # For serving\n",
    "\n",
    "print(\"Dependencies check...\")\n",
    "\n",
    "required_packages = [\n",
    "    \"torch\", \"transformers\", \"datasets\", \"accelerate\",\n",
    "    \"librosa\", \"soundfile\", \"pandas\", \"tqdm\", \"requests\"\n",
    "]\n",
    "\n",
    "missing = []\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "        print(f\"  [OK] {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"  [MISSING] {pkg}\")\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nPlease install missing packages: pip install {' '.join(missing)}\")\n",
    "else:\n",
    "    print(\"\\nAll required packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project configuration\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config import config, print_config, DATA_DIR, MODEL_DIR, OUTPUT_DIR\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - MODIFY THESE SETTINGS\n",
    "# ============================================================================\n",
    "\n",
    "# Toggle sample mode for quick testing (uses ~20GB instead of ~200GB)\n",
    "SAMPLE_MODE = True  # Set to False for full training\n",
    "SAMPLE_SIZE = 1000  # Samples per language in sample mode\n",
    "\n",
    "# Target languages\n",
    "LANGUAGES = [\"aka\", \"ewe\", \"gaa\", \"dag\"]  # Akan, Ewe, Ga, Dagbani\n",
    "\n",
    "# Update config\n",
    "config.dataset.sample_mode = SAMPLE_MODE\n",
    "config.dataset.sample_size = SAMPLE_SIZE\n",
    "config.dataset.languages = LANGUAGES\n",
    "\n",
    "# Print configuration\n",
    "print_config()\n",
    "\n",
    "print(f\"\\nSAMPLE_MODE: {SAMPLE_MODE}\")\n",
    "print(f\"SAMPLE_SIZE: {SAMPLE_SIZE}\")\n",
    "print(f\"LANGUAGES: {LANGUAGES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Dataset Download & Organization\n",
    "---\n",
    "\n",
    "This section downloads the required datasets:\n",
    "- **UGSpeechData**: ~5,400 hours of Ghanaian speech (for ASR)\n",
    "- **BibleTTS**: High-quality studio recordings (for TTS)\n",
    "- **FISD**: Financial inclusion speech dataset (for Ga and domain adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Initialize Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import DatasetDownloader\n",
    "\n",
    "# Create downloader instance\n",
    "downloader = DatasetDownloader()\n",
    "\n",
    "print(f\"Download directory: {downloader.output_dir}\")\n",
    "print(f\"Download directory exists: {downloader.output_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Download UGSpeechData (ASR Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download UGSpeechData - transcribed portions only (much smaller)\n",
    "# Full dataset is ~336GB, transcribed subset is ~50GB\n",
    "\n",
    "print(\"Downloading UGSpeechData (transcribed portions)...\")\n",
    "print(\"This will download metadata and transcribed audio for:\")\n",
    "print(\"  - Akan (~18,000 files, ~104 hours)\")\n",
    "print(\"  - Ewe (~19,000 files, ~106 hours)\")\n",
    "print(\"  - Dagbani (~similar size)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Uncomment to download\n",
    "# ugspeech_paths = downloader.download_ugspeechdata(\n",
    "#     languages=[\"akan\", \"ewe\", \"dagbani\"],\n",
    "#     transcribed_only=True  # Only download transcribed subset\n",
    "# )\n",
    "\n",
    "# For now, we'll set up the expected paths\n",
    "ugspeech_paths = {\n",
    "    \"akan\": DATA_DIR / \"raw\" / \"ugspeechdata\" / \"akan\",\n",
    "    \"ewe\": DATA_DIR / \"raw\" / \"ugspeechdata\" / \"ewe\",\n",
    "    \"dagbani\": DATA_DIR / \"raw\" / \"ugspeechdata\" / \"dagbani\"\n",
    "}\n",
    "\n",
    "print(\"Expected UGSpeechData paths:\")\n",
    "for lang, path in ugspeech_paths.items():\n",
    "    exists = \"[EXISTS]\" if path.exists() else \"[NOT FOUND]\"\n",
    "    print(f\"  {lang}: {path} {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Download BibleTTS (TTS Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download BibleTTS - high-quality studio recordings\n",
    "# Each language is ~15-20GB\n",
    "\n",
    "print(\"Downloading BibleTTS...\")\n",
    "print(\"High-quality single-speaker recordings for TTS training:\")\n",
    "print(\"  - Asante Twi (~15GB)\")\n",
    "print(\"  - Akuapem Twi (~16GB)\")\n",
    "print(\"  - Ewe (~19GB)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Uncomment to download\n",
    "# bibletis_paths = downloader.download_bibletis(\n",
    "#     languages=[\"asante_twi\", \"akuapem_twi\", \"ewe\"]\n",
    "# )\n",
    "\n",
    "# Expected paths\n",
    "bibletis_paths = {\n",
    "    \"asante_twi\": DATA_DIR / \"raw\" / \"bibletis\" / \"asante_twi\",\n",
    "    \"akuapem_twi\": DATA_DIR / \"raw\" / \"bibletis\" / \"akuapem_twi\",\n",
    "    \"ewe\": DATA_DIR / \"raw\" / \"bibletis\" / \"ewe\"\n",
    "}\n",
    "\n",
    "print(\"Expected BibleTTS paths:\")\n",
    "for lang, path in bibletis_paths.items():\n",
    "    exists = \"[EXISTS]\" if path.exists() else \"[NOT FOUND]\"\n",
    "    print(f\"  {lang}: {path} {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Download FISD (Ga Language Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FISD - Financial Inclusion Speech Dataset\n",
    "# Important for Ga language which is missing from BibleTTS\n",
    "\n",
    "print(\"Downloading FISD (Financial Inclusion Speech Dataset)...\")\n",
    "print(\"Telephony-quality speech for mobile applications:\")\n",
    "print(\"  - Ga (~148 hours total)\")\n",
    "print(\"  - Asante Twi\")\n",
    "print(\"  - Akuapim Twi\")\n",
    "print(\"  - Fante\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Uncomment to download\n",
    "# fisd_paths = downloader.download_fisd(\n",
    "#     languages=[\"ga\", \"asante_twi\"]\n",
    "# )\n",
    "\n",
    "# Expected paths\n",
    "fisd_paths = {\n",
    "    \"ga\": DATA_DIR / \"raw\" / \"fisd\" / \"ga\",\n",
    "    \"asante_twi\": DATA_DIR / \"raw\" / \"fisd\" / \"asante_twi\"\n",
    "}\n",
    "\n",
    "print(\"Expected FISD paths:\")\n",
    "for lang, path in fisd_paths.items():\n",
    "    exists = \"[EXISTS]\" if path.exists() else \"[NOT FOUND]\"\n",
    "    print(f\"  {lang}: {path} {exists}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def get_folder_size(path):\n",
    "    \"\"\"Calculate folder size in GB.\"\"\"\n",
    "    total = 0\n",
    "    if path.exists():\n",
    "        for f in path.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total / 1024**3\n",
    "\n",
    "def count_audio_files(path, extensions=[\".wav\", \".mp3\", \".flac\"]):\n",
    "    \"\"\"Count audio files in directory.\"\"\"\n",
    "    count = 0\n",
    "    if path.exists():\n",
    "        for ext in extensions:\n",
    "            count += len(list(path.rglob(f\"*{ext}\")))\n",
    "    return count\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "datasets = {\n",
    "    \"UGSpeechData\": DATA_DIR / \"raw\" / \"ugspeechdata\",\n",
    "    \"BibleTTS\": DATA_DIR / \"raw\" / \"bibletis\",\n",
    "    \"FISD\": DATA_DIR / \"raw\" / \"fisd\"\n",
    "}\n",
    "\n",
    "for name, path in datasets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Path: {path}\")\n",
    "    print(f\"  Exists: {path.exists()}\")\n",
    "    if path.exists():\n",
    "        size = get_folder_size(path)\n",
    "        files = count_audio_files(path)\n",
    "        print(f\"  Size: {size:.2f} GB\")\n",
    "        print(f\"  Audio Files: {files:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Processing & Preparation\n",
    "---\n",
    "\n",
    "This section processes the downloaded datasets into formats suitable for training:\n",
    "- **ASR**: 16kHz, mono, normalized audio + transcriptions\n",
    "- **TTS**: 22.05kHz, mono, trimmed audio + metadata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialize Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_processing import (\n",
    "    AudioProcessor, \n",
    "    ASRDatasetFormatter, \n",
    "    TTSDatasetFormatter\n",
    ")\n",
    "\n",
    "# Initialize processors\n",
    "audio_processor = AudioProcessor(\n",
    "    target_sr_asr=16000,  # 16kHz for ASR\n",
    "    target_sr_tts=22050   # 22.05kHz for TTS\n",
    ")\n",
    "\n",
    "asr_formatter = ASRDatasetFormatter(audio_processor)\n",
    "tts_formatter = TTSDatasetFormatter(audio_processor)\n",
    "\n",
    "print(\"Audio Processor initialized\")\n",
    "print(f\"  ASR sample rate: {audio_processor.target_sr_asr} Hz\")\n",
    "print(f\"  TTS sample rate: {audio_processor.target_sr_tts} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Process Audio for ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process UGSpeechData for ASR training\n",
    "# This resamples to 16kHz and normalizes audio\n",
    "\n",
    "from config import config\n",
    "\n",
    "asr_data_dir = config.dataset.asr_data_dir\n",
    "asr_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Processing audio for ASR training...\")\n",
    "print(f\"Output directory: {asr_data_dir}\")\n",
    "print(\"\\nThis step:\")\n",
    "print(\"  1. Resamples audio to 16kHz\")\n",
    "print(\"  2. Converts to mono\")\n",
    "print(\"  3. Normalizes amplitude\")\n",
    "print(\"  4. Saves as WAV format\")\n",
    "\n",
    "# Example processing (uncomment when data is downloaded)\n",
    "# for lang in [\"akan\", \"ewe\", \"dagbani\"]:\n",
    "#     input_dir = DATA_DIR / \"raw\" / \"ugspeechdata\" / lang / \"transcribed\"\n",
    "#     output_dir = asr_data_dir / lang / \"wavs\"\n",
    "#     \n",
    "#     if input_dir.exists():\n",
    "#         print(f\"\\nProcessing {lang}...\")\n",
    "#         processed = audio_processor.batch_process(\n",
    "#             input_dir=input_dir,\n",
    "#             output_dir=output_dir,\n",
    "#             mode=\"asr\",\n",
    "#             show_progress=True\n",
    "#         )\n",
    "#         print(f\"  Processed {len(processed)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Process Audio for TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process BibleTTS for TTS training\n",
    "# This creates the XTTS-compatible format\n",
    "\n",
    "tts_data_dir = config.dataset.tts_data_dir\n",
    "tts_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Processing audio for TTS training...\")\n",
    "print(f\"Output directory: {tts_data_dir}\")\n",
    "print(\"\\nThis step:\")\n",
    "print(\"  1. Resamples audio to 22.05kHz\")\n",
    "print(\"  2. Converts to mono\")\n",
    "print(\"  3. Trims leading/trailing silence\")\n",
    "print(\"  4. Creates metadata.csv in XTTS format\")\n",
    "print(\"\\nXTTS metadata format: filename|text|speaker|language\")\n",
    "\n",
    "# Example processing (uncomment when data is downloaded)\n",
    "# for lang in [\"asante_twi\", \"akuapem_twi\", \"ewe\"]:\n",
    "#     input_dir = DATA_DIR / \"raw\" / \"bibletis\" / lang\n",
    "#     \n",
    "#     if input_dir.exists():\n",
    "#         print(f\"\\nProcessing {lang} for TTS...\")\n",
    "#         \n",
    "#         # Parse BibleTTS structure\n",
    "#         df = tts_formatter.parse_bibletis_structure(input_dir, lang)\n",
    "#         \n",
    "#         # Sample if in sample mode\n",
    "#         if config.dataset.sample_mode and len(df) > config.dataset.sample_size:\n",
    "#             df = df.sample(n=config.dataset.sample_size, random_state=42)\n",
    "#         \n",
    "#         # Create XTTS dataset\n",
    "#         output_dir = tts_data_dir / lang\n",
    "#         tts_formatter.create_xtts_dataset(df, output_dir)\n",
    "#         \n",
    "#         print(f\"  Created TTS dataset with {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create Unified Multi-Language Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all language TTS datasets into one unified dataset\n",
    "# This is the \"One Model\" approach\n",
    "\n",
    "print(\"Creating unified multi-language TTS dataset...\")\n",
    "print(\"\\nThis combines all language datasets with distinct speakers:\")\n",
    "print(\"  - Twi_Speaker (from Asante/Akuapem Twi)\")\n",
    "print(\"  - Ewe_Speaker (from Ewe)\")\n",
    "print(\"  - Ga_Speaker (from FISD Ga)\")\n",
    "print(\"  - Dagbani_Speaker (from UGSpeechData)\")\n",
    "\n",
    "# Example merging (uncomment when individual datasets are ready)\n",
    "# tts_datasets = [\n",
    "#     tts_data_dir / \"asante_twi\",\n",
    "#     tts_data_dir / \"akuapem_twi\",\n",
    "#     tts_data_dir / \"ewe\",\n",
    "# ]\n",
    "# \n",
    "# existing_datasets = [d for d in tts_datasets if d.exists()]\n",
    "# \n",
    "# if existing_datasets:\n",
    "#     unified_dir = tts_data_dir / \"unified\"\n",
    "#     tts_formatter.merge_datasets(existing_datasets, unified_dir)\n",
    "#     print(f\"\\nUnified dataset created at: {unified_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Create HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset objects for ASR training\n",
    "from datasets import Dataset, Audio, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Creating HuggingFace Dataset objects...\")\n",
    "print(\"\\nThese will be used for training with the Trainer API.\")\n",
    "\n",
    "# Example dataset creation (uncomment when data is processed)\n",
    "# def create_asr_dataset(lang):\n",
    "#     \"\"\"Create HuggingFace dataset for a language.\"\"\"\n",
    "#     audio_dir = asr_data_dir / lang / \"wavs\"\n",
    "#     metadata_path = DATA_DIR / \"raw\" / \"ugspeechdata\" / lang / f\"{lang.capitalize()}.xlsx\"\n",
    "#     \n",
    "#     # Parse metadata\n",
    "#     df = asr_formatter.parse_ugspeechdata_metadata(\n",
    "#         metadata_path, audio_dir, lang\n",
    "#     )\n",
    "#     \n",
    "#     # Create dataset\n",
    "#     dataset = asr_formatter.create_hf_dataset(df)\n",
    "#     \n",
    "#     # Split into train/val/test\n",
    "#     splits = asr_formatter.create_train_val_test_split(dataset)\n",
    "#     \n",
    "#     return splits\n",
    "# \n",
    "# # Create datasets for each language\n",
    "# asr_datasets = {}\n",
    "# for lang in [\"akan\", \"ewe\", \"dagbani\"]:\n",
    "#     try:\n",
    "#         asr_datasets[lang] = create_asr_dataset(lang)\n",
    "#         print(f\"  Created dataset for {lang}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"  Error creating dataset for {lang}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Model Training\n",
    "---\n",
    "\n",
    "This section covers training/fine-tuning the three components:\n",
    "- **3A**: ASR - The Ear (Meta MMS)\n",
    "- **3B**: TTS - The Mouth (XTTS v2)\n",
    "- **3C**: Translation - The Brain (NLLB-200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3A: ASR Training (The Ear)\n",
    "\n",
    "Fine-tune Meta's MMS (Massively Multilingual Speech) model using LoRA adapters for memory efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.1 Load MMS Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, AutoProcessor, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Model ID\n",
    "ASR_MODEL_ID = \"facebook/mms-1b-all\"\n",
    "\n",
    "print(f\"Loading ASR model: {ASR_MODEL_ID}\")\n",
    "print(\"Using 8-bit quantization for memory efficiency...\")\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "asr_processor = AutoProcessor.from_pretrained(ASR_MODEL_ID)\n",
    "\n",
    "# Load model with quantization\n",
    "asr_model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    ASR_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "print(f\"Model parameters: {asr_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.2 Apply LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# LoRA configuration\n",
    "# This makes only ~1% of parameters trainable\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    inference_mode=False,\n",
    "    r=16,              # Rank of update matrices\n",
    "    lora_alpha=32,     # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Target attention layers\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "asr_model = get_peft_model(asr_model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "asr_model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nLoRA adapters applied!\")\n",
    "print(\"Only attention layers will be fine-tuned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.3 Set Target Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target language for training\n",
    "# MMS uses ISO 639-3 codes\n",
    "TARGET_LANG = \"aka\"  # Akan\n",
    "\n",
    "print(f\"Setting target language: {TARGET_LANG}\")\n",
    "\n",
    "# Set language in processor\n",
    "asr_processor.tokenizer.set_target_lang(TARGET_LANG)\n",
    "\n",
    "# Load language adapter\n",
    "asr_model.load_adapter(TARGET_LANG)\n",
    "\n",
    "print(f\"Language adapter loaded for: {TARGET_LANG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.4 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for RTX 3090/4090\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(MODEL_DIR / \"asr\" / \"checkpoints\"),\n",
    "    \n",
    "    # Batch size settings\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=100,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=5,\n",
    "    max_steps=2000,  # Override epochs if set\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Other\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Max steps: {training_args.max_steps}\")\n",
    "print(f\"  FP16: {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.5 Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator for CTC-based ASR training.\n",
    "    Pads inputs and labels to the maximum length in the batch.\n",
    "    \"\"\"\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: int = 16000 * 30  # 30 seconds max\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Separate inputs and labels\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        \n",
    "        # Pad inputs\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Pad labels\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        # Replace padding with -100 for CTC loss\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorCTCWithPadding(processor=asr_processor)\n",
    "print(\"Data collator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.6 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load WER metric\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    pred.label_ids[pred.label_ids == -100] = asr_processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and references\n",
    "    pred_str = asr_processor.batch_decode(pred_ids)\n",
    "    label_str = asr_processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "print(\"WER metric configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A.7 Train ASR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ASR TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo train the ASR model, you need to:\")\n",
    "print(\"1. Download and process the datasets (Parts 1-2)\")\n",
    "print(\"2. Create train/eval datasets\")\n",
    "print(\"3. Uncomment and run the training code below\")\n",
    "\n",
    "# Uncomment to train (requires datasets to be ready)\n",
    "# trainer = Trainer(\n",
    "#     model=asr_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=asr_processor.feature_extractor,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "# \n",
    "# # Train\n",
    "# trainer.train()\n",
    "# \n",
    "# # Save model\n",
    "# trainer.save_model(str(MODEL_DIR / \"asr\" / \"final\"))\n",
    "# print(\"\\nASR model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3B: TTS Training (The Mouth)\n",
    "\n",
    "Fine-tune XTTS v2 to speak Ghanaian languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.1 Install Coqui TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Coqui TTS if not already installed\n",
    "# !pip install TTS\n",
    "\n",
    "# Install espeak-ng for phonemization\n",
    "# Linux: sudo apt-get install espeak-ng\n",
    "# Windows: Download from https://github.com/espeak-ng/espeak-ng/releases\n",
    "\n",
    "try:\n",
    "    from TTS.api import TTS\n",
    "    print(\"Coqui TTS imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Please install Coqui TTS: pip install TTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.2 Download Base XTTS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.utils.manage import ModelManager\n",
    "import os\n",
    "\n",
    "# XTTS v2 model name\n",
    "TTS_MODEL_NAME = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
    "\n",
    "print(f\"Downloading base XTTS v2 model...\")\n",
    "print(\"This will download ~2-3GB on first run.\")\n",
    "\n",
    "# Download model\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Check if already downloaded\n",
    "model_path = os.path.join(\n",
    "    model_manager.output_prefix,\n",
    "    TTS_MODEL_NAME.replace(\"/\", \"--\")\n",
    ")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading model...\")\n",
    "    # model_manager.download_model(TTS_MODEL_NAME)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Model already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.3 TTS Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_PATH = str(config.dataset.tts_data_dir / \"unified\")\n",
    "OUTPUT_PATH = str(MODEL_DIR / \"tts\")\n",
    "\n",
    "# Create configs\n",
    "print(\"TTS Training Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Output path: {OUTPUT_PATH}\")\n",
    "print(\"\\nTraining settings:\")\n",
    "print(f\"  Batch size: 2 (small for memory)\")\n",
    "print(f\"  Epochs: 10\")\n",
    "print(f\"  Learning rate: 5e-6\")\n",
    "print(f\"  Carrier language: 'en' (English base)\")\n",
    "print(\"\\nWhat gets trained:\")\n",
    "print(f\"  GPT layers: Yes (learns language patterns)\")\n",
    "print(f\"  HiFi-GAN vocoder: No (already good)\")\n",
    "print(f\"  Speaker encoder: No (uses cloning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B.4 Train TTS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TTS TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo train the TTS model, you need to:\")\n",
    "print(\"1. Download and process BibleTTS datasets\")\n",
    "print(\"2. Create unified TTS dataset with metadata.csv\")\n",
    "print(\"3. Uncomment and run the training code below\")\n",
    "\n",
    "# Uncomment to train (requires datasets to be ready)\n",
    "# \n",
    "# # Load config\n",
    "# config = XttsConfig()\n",
    "# config.load_json(f\"{model_path}/config.json\")\n",
    "# \n",
    "# # Configure dataset\n",
    "# config.dataset_config = BaseDatasetConfig(\n",
    "#     formatter=\"coqui\",\n",
    "#     dataset_name=\"ghana_unified\",\n",
    "#     path=DATASET_PATH,\n",
    "#     meta_file_train=\"metadata.csv\",\n",
    "#     language=\"en\"\n",
    "# )\n",
    "# \n",
    "# # Training settings\n",
    "# config.batch_size = 2\n",
    "# config.epochs = 10\n",
    "# config.lr = 5e-6\n",
    "# config.output_path = OUTPUT_PATH\n",
    "# \n",
    "# # What to train\n",
    "# config.train_gpt = True\n",
    "# config.train_hifi_gan = False\n",
    "# config.train_speaker_encoder = False\n",
    "# \n",
    "# # Initialize model\n",
    "# model = Xtts.init_from_config(config)\n",
    "# model.load_checkpoint(config, checkpoint_dir=model_path, eval=True)\n",
    "# \n",
    "# # Train\n",
    "# trainer = Trainer(\n",
    "#     TrainerArgs(),\n",
    "#     config,\n",
    "#     output_path=OUTPUT_PATH,\n",
    "#     model=model,\n",
    "# )\n",
    "# \n",
    "# trainer.fit()\n",
    "# print(\"\\nTTS training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3C: Translation Setup (The Brain)\n",
    "\n",
    "Set up NLLB-200 for translation between languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C.1 Load NLLB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# NLLB model - using distilled version for speed\n",
    "MT_MODEL_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "# For better quality, use: \"facebook/nllb-200-3.3B\"\n",
    "\n",
    "print(f\"Loading translation model: {MT_MODEL_ID}\")\n",
    "\n",
    "mt_tokenizer = AutoTokenizer.from_pretrained(MT_MODEL_ID)\n",
    "mt_model = AutoModelForSeq2SeqLM.from_pretrained(MT_MODEL_ID).to(device)\n",
    "\n",
    "print(f\"Translation model loaded!\")\n",
    "print(f\"Parameters: {mt_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C.2 Language Codes Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLLB language codes for Ghanaian languages\n",
    "NLLB_CODES = {\n",
    "    \"Akan (Twi)\": \"aka_Latn\",\n",
    "    \"Ewe\": \"ewe_Latn\",\n",
    "    \"Ga\": \"gaa_Latn\",\n",
    "    \"Dagbani\": \"dag_Latn\",\n",
    "    \"English\": \"eng_Latn\",\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"Hausa\": \"hau_Latn\",\n",
    "}\n",
    "\n",
    "print(\"NLLB Language Codes:\")\n",
    "print(\"=\" * 40)\n",
    "for lang, code in NLLB_CODES.items():\n",
    "    print(f\"  {lang:15} -> {code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C.3 Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, source_lang=\"aka_Latn\", target_lang=\"eng_Latn\"):\n",
    "    \"\"\"\n",
    "    Translate text between languages using NLLB.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to translate\n",
    "        source_lang: Source language NLLB code\n",
    "        target_lang: Target language NLLB code\n",
    "    \n",
    "    Returns:\n",
    "        Translated text\n",
    "    \"\"\"\n",
    "    # Set source language\n",
    "    mt_tokenizer.src_lang = source_lang\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = mt_tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get target language token ID\n",
    "    target_token_id = mt_tokenizer.convert_tokens_to_ids(target_lang)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = mt_model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=target_token_id,\n",
    "            max_length=200,\n",
    "            num_beams=4\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    translation = mt_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return translation\n",
    "\n",
    "print(\"Translation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C.4 Test Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translations\n",
    "print(\"Testing Translation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# English to Twi\n",
    "english_text = \"Good morning, how are you today?\"\n",
    "twi_translation = translate(english_text, \"eng_Latn\", \"aka_Latn\")\n",
    "print(f\"\\nEnglish: {english_text}\")\n",
    "print(f\"Twi:     {twi_translation}\")\n",
    "\n",
    "# Twi to English (if you have Twi text)\n",
    "# twi_text = \"Maakye, wo ho te sen?\"\n",
    "# english_back = translate(twi_text, \"aka_Latn\", \"eng_Latn\")\n",
    "# print(f\"\\nTwi:     {twi_text}\")\n",
    "# print(f\"English: {english_back}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Unified Pipeline & Inference\n",
    "---\n",
    "\n",
    "Combine all components into a single Speech-to-Speech pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Initialize Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    "source": [
     "from utils.pipeline import GhanaS2SPipeline\n",
     "\n",
     "print(\"Initializing Ghana S2S Pipeline...\")\n",
     "print(\"This will load all models (LID, ASR, Translation, TTS)\")\n",
     "print(\"\\nNote: This requires ~15-20GB GPU memory\")\n",
     "\n",
     "# Initialize pipeline\n",
     "# Set paths to fine-tuned models if available\n",
     "pipeline = GhanaS2SPipeline(\n",
     "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
     "    asr_model_path=None,  # Use base MMS (or path to fine-tuned)\n",
     "    tts_model_path=None,  # Use base XTTS (or path to fine-tuned)\n",
     "    load_lid=True,        # Enable automatic language detection\n",
     "    load_asr=True,\n",
     "    load_tts=True,\n",
     "    load_translation=True,\n",
     "    use_8bit=True  # Memory optimization\n",
     ")"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Test Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ASR (requires audio file)\n",
    "# result = pipeline.listen(\"test_audio.wav\", language=\"aka\")\n",
    "# print(f\"Transcription: {result.text}\")\n",
    "\n",
    "# Test Translation\n",
    "result = pipeline.think(\"Hello, how are you?\", source_lang=\"eng\", target_lang=\"aka\")\n",
    "print(f\"Translation: {result.translated_text}\")\n",
    "\n",
    "# Test TTS\n",
    "result = pipeline.speak(\n",
    "    text=\"Good morning, welcome to Ghana!\",\n",
    "    output_path=\"test_output.wav\",\n",
    "    speaker=\"Twi_Speaker\"\n",
    ")\n",
    "print(f\"Audio saved to: {result.audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Full Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Speech-to-Speech pipeline\n",
    "# This requires an input audio file\n",
    "\n",
    "print(\"Full S2S Pipeline Demo\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo run the full pipeline:\")\n",
    "print(\"1. Record or upload an audio file\")\n",
    "print(\"2. Specify source and target languages\")\n",
    "print(\"3. Optionally provide a speaker reference for voice cloning\")\n",
    "\n",
    "# Example usage (uncomment with actual audio file)\n",
    "# result = pipeline.run_pipeline(\n",
    "#     audio_input=\"input_english.wav\",\n",
    "#     source_lang=\"eng\",\n",
    "#     target_lang=\"aka\",  # Translate to Twi\n",
    "#     speaker_ref=None,   # Optional: 6-second reference clip\n",
    "#     translate=True\n",
    "# )\n",
    "# \n",
    "# print(f\"\\nResults:\")\n",
    "# print(f\"  Transcription: {result.transcription.text}\")\n",
    "# print(f\"  Translation: {result.translation.translated_text}\")\n",
    "# print(f\"  Output audio: {result.synthesis.audio_path}\")\n",
    "# print(f\"  Total latency: {result.total_latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Audio Recording Widget (for Notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio recording for Jupyter/Colab\n",
    "# This creates a \"Record\" button in the notebook\n",
    "\n",
    "try:\n",
    "    from IPython.display import Javascript, Audio\n",
    "    from google.colab import output\n",
    "    from base64 import b64decode\n",
    "    \n",
    "    RECORD_JS = \"\"\"\n",
    "    const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "    const b2text = blob => new Promise(resolve => {\n",
    "      const reader = new FileReader()\n",
    "      reader.onloadend = e => resolve(e.srcElement.result)\n",
    "      reader.readAsDataURL(blob)\n",
    "    })\n",
    "    var record = time => new Promise(async resolve => {\n",
    "      stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "      recorder = new MediaRecorder(stream)\n",
    "      chunks = []\n",
    "      recorder.ondataavailable = e => chunks.push(e.data)\n",
    "      recorder.start()\n",
    "      await sleep(time)\n",
    "      recorder.onstop = async ()=>{\n",
    "        blob = new Blob(chunks)\n",
    "        text = await b2text(blob)\n",
    "        resolve(text)\n",
    "      }\n",
    "      recorder.stop()\n",
    "    })\n",
    "    \"\"\"\n",
    "    \n",
    "    def record_audio(seconds=5):\n",
    "        \"\"\"Record audio for specified seconds.\"\"\"\n",
    "        display(Javascript(RECORD_JS))\n",
    "        s = output.eval_js(f'record({seconds * 1000})')\n",
    "        b = b64decode(s.split(',')[1])\n",
    "        \n",
    "        with open('recorded_audio.wav', 'wb') as f:\n",
    "            f.write(b)\n",
    "        \n",
    "        return 'recorded_audio.wav'\n",
    "    \n",
    "    print(\"Audio recording available! Use record_audio(seconds) to record.\")\n",
    "    \n",
    "except:\n",
     "    print(\"Audio recording not available in this environment.\")\n",
     "    print(\"Please upload audio files manually.\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 4.5 Automatic Language Detection"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Automatic Language Detection using MMS-LID\n",
     "# The pipeline can automatically detect which Ghanaian language is being spoken\n",
     "\n",
     "# Example 1: Detect language from audio file\n",
     "# audio_path = \"test_audio.wav\"\n",
     "# detected_lang = pipeline.detect_language(audio_path)\n",
     "# print(f\"Detected language: {detected_lang}\")\n",
     "\n",
     "# Example 2: Get confidence scores for top languages\n",
     "# scores = pipeline.detect_language(audio_path, top_k=5, return_all_scores=True)\n",
     "# for lang, score in scores.items():\n",
     "#     print(f\"  {lang}: {score:.2%}\")\n",
     "\n",
     "# Example 3: Transcribe with auto-detection\n",
     "# result = pipeline.listen(\"audio.wav\", language=\"auto\")\n",
     "# print(f\"Detected: {result.language}, Text: {result.text}\")\n",
     "\n",
     "# Example 4: Full S2S pipeline with auto-detection\n",
     "# result = pipeline.run_pipeline(\n",
     "#     audio_input=\"input.wav\",\n",
     "#     source_lang=\"auto\",  # Auto-detect input language\n",
     "#     target_lang=\"eng\"    # Translate to English\n",
     "# )\n",
     "\n",
     "print(\"Language Detection Available!\")\n",
     "print(\"\\nSupported languages for auto-detection:\")\n",
     "for code, name in pipeline.SUPPORTED_LANGUAGES.items():\n",
     "    print(f\"  {code}: {name}\")\n",
     "print(\"\\nUsage: pipeline.detect_language('audio.wav')\")\n",
     "print(\"   or: pipeline.listen('audio.wav', language='auto')\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "---\n",
     "# Part 5: Evaluation & Benchmarking\n",
     "---"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ASR Evaluation (WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "def evaluate_asr(predictions, references):\n",
    "    \"\"\"\n",
    "    Evaluate ASR performance.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of predicted transcriptions\n",
    "        references: List of ground truth transcriptions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with WER and CER scores\n",
    "    \"\"\"\n",
    "    wer = wer_metric.compute(predictions=predictions, references=references)\n",
    "    cer = cer_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return {\n",
    "        \"wer\": wer * 100,  # Percentage\n",
    "        \"cer\": cer * 100\n",
    "    }\n",
    "\n",
    "print(\"ASR Evaluation Metrics:\")\n",
    "print(\"  - WER (Word Error Rate): Lower is better\")\n",
    "print(\"  - CER (Character Error Rate): Lower is better\")\n",
    "print(\"\\nTarget: WER < 20% for production quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Translation Evaluation (BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def evaluate_translation(predictions, references):\n",
    "    \"\"\"\n",
    "    Evaluate translation quality using BLEU score.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of translated texts\n",
    "        references: List of reference translations (can be list of lists)\n",
    "    \n",
    "    Returns:\n",
    "        BLEU score\n",
    "    \"\"\"\n",
    "    # Ensure references are in correct format\n",
    "    if isinstance(references[0], str):\n",
    "        references = [[ref] for ref in references]\n",
    "    \n",
    "    bleu = corpus_bleu(predictions, references)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu.score,\n",
    "        \"precisions\": bleu.precisions\n",
    "    }\n",
    "\n",
    "print(\"Translation Evaluation Metrics:\")\n",
    "print(\"  - BLEU Score: Higher is better (0-100)\")\n",
    "print(\"\\nTarget: BLEU > 20 for reasonable quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 End-to-End Latency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_pipeline(pipeline, audio_path, num_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark pipeline latency.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: GhanaS2SPipeline instance\n",
    "        audio_path: Path to test audio\n",
    "        num_runs: Number of runs for averaging\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with latency statistics\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        start = time.time()\n",
    "        \n",
    "        result = pipeline.run_pipeline(\n",
    "            audio_input=audio_path,\n",
    "            source_lang=\"eng\",\n",
    "            target_lang=\"aka\",\n",
    "            translate=True\n",
    "        )\n",
    "        \n",
    "        latencies.append(time.time() - start)\n",
    "    \n",
    "    return {\n",
    "        \"mean_latency\": sum(latencies) / len(latencies),\n",
    "        \"min_latency\": min(latencies),\n",
    "        \"max_latency\": max(latencies)\n",
    "    }\n",
    "\n",
    "print(\"Latency Benchmarking:\")\n",
    "print(\"  - Target: < 3 seconds for interactive use\")\n",
    "print(\"  - Acceptable: < 10 seconds for offline processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Deployment & Serving\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6A: Gradio Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.serving import create_gradio_interface, launch_gradio\n",
    "\n",
    "print(\"Launching Gradio Interface...\")\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"  - Microphone recording\")\n",
    "print(\"  - File upload\")\n",
    "print(\"  - Language selection\")\n",
    "print(\"  - Voice cloning with reference audio\")\n",
    "\n",
    "# Launch Gradio (uncomment to run)\n",
    "# interface = create_gradio_interface(pipeline)\n",
    "# interface.launch(\n",
    "#     share=False,  # Set True for public link\n",
    "#     server_port=7860\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B: FastAPI REST Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.serving import create_fastapi_app\n",
    "\n",
    "print(\"FastAPI Endpoints:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAvailable endpoints:\")\n",
    "print(\"  POST /api/transcribe    - Speech to text\")\n",
    "print(\"  POST /api/translate     - Text translation\")\n",
    "print(\"  POST /api/synthesize    - Text to speech\")\n",
    "print(\"  POST /api/speech-to-speech - Full pipeline\")\n",
    "print(\"  GET  /api/languages     - List supported languages\")\n",
    "print(\"  GET  /health            - Health check\")\n",
    "\n",
    "# Create app (for running with uvicorn)\n",
    "# app = create_fastapi_app(pipeline)\n",
    "\n",
    "# To run: uvicorn utils.serving:create_fastapi_app --host 0.0.0.0 --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6C: Docker Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
     "from utils.serving import generate_docker_files\\n",
     "\\n",
     "print(\"Generating Docker Configuration...\")\\n",
     "\\n",
     "# Generate Docker files\\n",
     "# generate_docker_files()\\n",
     "\\n",
     "print(\"\\nDocker commands:\")\\n",
     "print(\"  Build:  docker-compose build\")\\n",
     "print(\"  Run:    docker-compose up\")\\n",
     "print(\"  Stop:   docker-compose down\")\\n",
     "print(\"\\nThe API will be available at:\")\\n",
     "print(\"  - REST API: http://localhost:8000\")\\n",
     "print(\"  - Gradio:   http://localhost:7860\")"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n",
    "# Part 7: Upload Models to HuggingFace\\n",
    "---\\n",
    "\\n",
    "Share your trained models with the community by uploading to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Install HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface_hub if not already installed\\n",
    "# !pip install huggingface_hub\\n",
    "\\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_folder\\n",
    "from huggingface_hub import login as hf_login\\n",
    "import os\\n",
    "from pathlib import Path\\n",
    "\\n",
    "print(\"HuggingFace Hub imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Authenticate with HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\\n",
    "# You need a HuggingFace account and access token\\n",
    "# Get your token from: https://huggingface.co/settings/tokens\\n",
    "\\n",
    "# Option 1: Interactive login (will prompt for token)\\n",
    "# hf_login()\\n",
    "\\n",
    "# Option 2: Login with token directly\\n",
    "# hf_login(token=\"your_token_here\")\\n",
    "\\n",
    "# Option 3: Set environment variable\\n",
    "# os.environ[\"HF_TOKEN\"] = \"your_token_here\"\\n",
    "\\n",
    "print(\"Authentication options:\")\\n",
    "print(\"  1. Run: hf_login() - Interactive prompt\")\\n",
    "print(\"  2. Run: hf_login(token='hf_xxx') - Direct token\")\\n",
    "print(\"  3. Set: os.environ['HF_TOKEN'] = 'hf_xxx'\")\\n",
    "print(\"\\nGet your token at: https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Configure Upload Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n",
    "# UPLOAD CONFIGURATION - MODIFY THESE\\n",
    "# ============================================================================\\n",
    "\\n",
    "# Your HuggingFace username or organization\\n",
    "HF_USERNAME = \"your-username\"  # Change this!\\n",
    "\\n",
    "# Repository names for each model\\n",
    "ASR_REPO_NAME = \"ghana-asr-mms-akan-ewe-ga-dagbani\"\\n",
    "TTS_REPO_NAME = \"ghana-tts-xtts-akan-ewe-ga-dagbani\"\\n",
    "\\n",
    "# Local model paths\\n",
    "ASR_MODEL_PATH = MODEL_DIR / \"asr\" / \"final\"\\n",
    "TTS_MODEL_PATH = MODEL_DIR / \"tts\"  # Will look for latest checkpoint\\n",
    "\\n",
    "# Model metadata\\n",
    "MODEL_CARD_LANGUAGES = [\"ak\", \"ee\", \"gaa\", \"dag\", \"en\"]  # ISO 639-1 codes\\n",
    "MODEL_TAGS = [\\n",
    "    \"speech-recognition\",\\n",
    "    \"text-to-speech\",\\n",
    "    \"akan\",\\n",
    "    \"twi\",\\n",
    "    \"ewe\",\\n",
    "    \"ga\",\\n",
    "    \"dagbani\",\\n",
    "    \"ghana\",\\n",
    "    \"african-languages\",\\n",
    "    \"low-resource\",\\n",
    "]\\n",
    "\\n",
    "print(f\"ASR Repo: {HF_USERNAME}/{ASR_REPO_NAME}\")\\n",
    "print(f\"TTS Repo: {HF_USERNAME}/{TTS_REPO_NAME}\")\\n",
    "print(f\"\\nASR Model Path: {ASR_MODEL_PATH}\")\\n",
    "print(f\"TTS Model Path: {TTS_MODEL_PATH}\")"
   ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7.4 Create Model Cards"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "def create_asr_model_card(repo_name: str, languages: list) -> str:\n",
     "    \"\"\"Create a model card for the ASR model.\"\"\"\n",
     "    lang_yaml = '\\n'.join([f'- {lang}' for lang in languages])\n",
     "    model_card = f\"\"\"---\n",
     "language:\n",
     "{lang_yaml}\n",
     "license: cc-by-nc-4.0\n",
     "tags:\n",
     "- automatic-speech-recognition\n",
     "- mms\n",
     "- akan\n",
     "- twi\n",
     "- ewe\n",
     "- ga\n",
     "- dagbani\n",
     "- ghana\n",
     "- african-languages\n",
     "- pytorch\n",
     "- lora\n",
     "datasets:\n",
     "- UGSpeechData\n",
     "base_model: facebook/mms-1b-all\n",
     "pipeline_tag: automatic-speech-recognition\n",
     "---\n",
     "\n",
     "# Ghana ASR: Multilingual Speech Recognition for Ghanaian Languages\n",
     "\n",
     "This model is a fine-tuned version of [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all) for Ghanaian languages.\n",
     "\n",
     "## Supported Languages\n",
     "\n",
     "| Language | Code | Hours Trained |\n",
     "|----------|------|---------------|\n",
     "| Akan (Twi/Fante) | aka | ~100 |\n",
     "| Ewe | ewe | ~100 |\n",
     "| Ga | gaa | ~50 |\n",
     "| Dagbani | dag | ~100 |\n",
     "\n",
     "## Usage\n",
     "\n",
     "```python\n",
     "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
     "import torch\n",
     "import librosa\n",
     "\n",
     "model = Wav2Vec2ForCTC.from_pretrained(\"{HF_USERNAME}/{repo_name}\")\n",
     "processor = AutoProcessor.from_pretrained(\"{HF_USERNAME}/{repo_name}\")\n",
     "\n",
     "# Set target language\n",
     "processor.tokenizer.set_target_lang(\"aka\")  # Akan\n",
     "model.load_adapter(\"aka\")\n",
     "\n",
     "# Load audio\n",
     "audio, sr = librosa.load(\"audio.wav\", sr=16000)\n",
     "inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
     "\n",
     "# Transcribe\n",
     "with torch.no_grad():\n",
     "    logits = model(**inputs).logits\n",
     "predicted_ids = torch.argmax(logits, dim=-1)\n",
     "transcription = processor.batch_decode(predicted_ids)[0]\n",
     "print(transcription)\n",
     "```\n",
     "\n",
     "## Training\n",
     "\n",
     "- **Base Model**: facebook/mms-1b-all\n",
     "- **Fine-tuning Method**: LoRA adapters\n",
     "- **Dataset**: UGSpeechData (University of Ghana)\n",
     "- **Training Hardware**: NVIDIA RTX 3090/4090\n",
     "\n",
     "## Citation\n",
     "\n",
     "If you use this model, please cite:\n",
     "\n",
     "```bibtex\n",
     "@misc{{ghana-asr,\n",
     "  title={{Ghana ASR: Multilingual Speech Recognition for Ghanaian Languages}},\n",
     "  year={{2024}},\n",
     "  url={{https://huggingface.co/{HF_USERNAME}/{repo_name}}}\n",
     "}}\n",
     "```\n",
     "\"\"\"\n",
     "    return model_card\n",
     "\n",
     "\n",
     "def create_tts_model_card(repo_name: str, languages: list) -> str:\n",
     "    \"\"\"Create a model card for the TTS model.\"\"\"\n",
     "    lang_yaml = '\\n'.join([f'- {lang}' for lang in languages])\n",
     "    model_card = f\"\"\"---\n",
     "language:\n",
     "{lang_yaml}\n",
     "license: cc-by-nc-4.0\n",
     "tags:\n",
     "- text-to-speech\n",
     "- xtts\n",
     "- akan\n",
     "- twi\n",
     "- ewe\n",
     "- ga\n",
     "- dagbani\n",
     "- ghana\n",
     "- african-languages\n",
     "- voice-cloning\n",
     "datasets:\n",
     "- BibleTTS\n",
     "base_model: coqui/XTTS-v2\n",
     "pipeline_tag: text-to-speech\n",
     "---\n",
     "\n",
     "# Ghana TTS: Multilingual Text-to-Speech for Ghanaian Languages\n",
     "\n",
     "This model is a fine-tuned version of XTTS v2 for Ghanaian languages.\n",
     "\n",
     "## Supported Languages/Speakers\n",
     "\n",
     "| Speaker Name | Language |\n",
     "|--------------|----------|\n",
     "| Twi_Speaker | Akan (Twi) |\n",
     "| Ewe_Speaker | Ewe |\n",
     "| Ga_Speaker | Ga |\n",
     "| Dagbani_Speaker | Dagbani |\n",
     "\n",
     "## Usage\n",
     "\n",
     "```python\n",
     "from TTS.api import TTS\n",
     "\n",
     "# Load model\n",
     "tts = TTS(model_path=\"{HF_USERNAME}/{repo_name}\")\n",
     "\n",
     "# Generate speech\n",
     "tts.tts_to_file(\n",
     "    text=\"Maakye! Wo ho te sen?\",\n",
     "    speaker=\"Twi_Speaker\",\n",
     "    language=\"en\",  # Carrier language\n",
     "    file_path=\"output.wav\"\n",
     ")\n",
     "```\n",
     "\n",
     "## Training\n",
     "\n",
     "- **Base Model**: XTTS v2\n",
     "- **Dataset**: BibleTTS (OpenSLR)\n",
     "- **Training Hardware**: NVIDIA RTX 3090/4090\n",
     "\n",
     "## Citation\n",
     "\n",
     "```bibtex\n",
     "@misc{{ghana-tts,\n",
     "  title={{Ghana TTS: Multilingual Text-to-Speech for Ghanaian Languages}},\n",
     "  year={{2024}},\n",
     "  url={{https://huggingface.co/{HF_USERNAME}/{repo_name}}}\n",
     "}}\n",
     "```\n",
     "\"\"\"\n",
     "    return model_card\n",
     "\n",
     "\n",
     "print(\"Model card templates created!\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7.5 Upload ASR Model"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "def upload_asr_model(\n",
     "    model_path: Path,\n",
     "    repo_name: str,\n",
     "    username: str,\n",
     "    private: bool = False\n",
     "):\n",
     "    \"\"\"\n",
     "    Upload ASR model to HuggingFace Hub.\n",
     "    \n",
     "    Args:\n",
     "        model_path: Path to trained model directory\n",
     "        repo_name: Repository name on HuggingFace\n",
     "        username: HuggingFace username or organization\n",
     "        private: Whether to make the repo private\n",
     "    \"\"\"\n",
     "    from huggingface_hub import HfApi, create_repo, upload_folder\n",
     "    \n",
     "    repo_id = f\"{username}/{repo_name}\"\n",
     "    \n",
     "    print(f\"Uploading ASR model to: {repo_id}\")\n",
     "    print(f\"Model path: {model_path}\")\n",
     "    \n",
     "    # Check if model exists\n",
     "    if not model_path.exists():\n",
     "        print(f\"ERROR: Model not found at {model_path}\")\n",
     "        print(\"Please train the model first (Part 3A).\")\n",
     "        return None\n",
     "    \n",
     "    # Create repository\n",
     "    try:\n",
     "        create_repo(repo_id, private=private, exist_ok=True)\n",
     "        print(f\"Repository created/exists: {repo_id}\")\n",
     "    except Exception as e:\n",
     "        print(f\"Error creating repo: {e}\")\n",
     "        return None\n",
     "    \n",
     "    # Create and save model card\n",
     "    model_card = create_asr_model_card(repo_name, MODEL_CARD_LANGUAGES)\n",
     "    readme_path = model_path / \"README.md\"\n",
     "    with open(readme_path, \"w\") as f:\n",
     "        f.write(model_card)\n",
     "    print(\"Model card created\")\n",
     "    \n",
     "    # Upload folder\n",
     "    api = HfApi()\n",
     "    api.upload_folder(\n",
     "        folder_path=str(model_path),\n",
     "        repo_id=repo_id,\n",
     "        repo_type=\"model\",\n",
     "    )\n",
     "    \n",
     "    print(f\"\\nUpload complete!\")\n",
     "    print(f\"View model at: https://huggingface.co/{repo_id}\")\n",
     "    return repo_id\n",
     "\n",
     "\n",
     "# Uncomment to upload\n",
     "# upload_asr_model(ASR_MODEL_PATH, ASR_REPO_NAME, HF_USERNAME)\n",
     "\n",
     "print(\"ASR upload function ready.\")\n",
     "print(\"Run: upload_asr_model(ASR_MODEL_PATH, ASR_REPO_NAME, HF_USERNAME)\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7.6 Upload TTS Model"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "def upload_tts_model(\n",
     "    model_path: Path,\n",
     "    repo_name: str,\n",
     "    username: str,\n",
     "    private: bool = False\n",
     "):\n",
     "    \"\"\"\n",
     "    Upload TTS model to HuggingFace Hub.\n",
     "    \n",
     "    Args:\n",
     "        model_path: Path to trained model directory\n",
     "        repo_name: Repository name on HuggingFace\n",
     "        username: HuggingFace username or organization\n",
     "        private: Whether to make the repo private\n",
     "    \"\"\"\n",
     "    from huggingface_hub import HfApi, create_repo, upload_folder\n",
     "    import glob\n",
     "    \n",
     "    repo_id = f\"{username}/{repo_name}\"\n",
     "    \n",
     "    print(f\"Uploading TTS model to: {repo_id}\")\n",
     "    \n",
     "    # Find the latest checkpoint\n",
     "    if not model_path.exists():\n",
     "        print(f\"ERROR: Model directory not found at {model_path}\")\n",
     "        print(\"Please train the model first (Part 3B).\")\n",
     "        return None\n",
     "    \n",
     "    # Look for checkpoint directories (XTTS saves with timestamps)\n",
     "    checkpoints = list(model_path.glob(\"run-*\"))\n",
     "    if checkpoints:\n",
     "        latest_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)\n",
     "        print(f\"Found checkpoint: {latest_checkpoint}\")\n",
     "        upload_path = latest_checkpoint\n",
     "    else:\n",
     "        upload_path = model_path\n",
     "    \n",
     "    # Create repository\n",
     "    try:\n",
     "        create_repo(repo_id, private=private, exist_ok=True)\n",
     "        print(f\"Repository created/exists: {repo_id}\")\n",
     "    except Exception as e:\n",
     "        print(f\"Error creating repo: {e}\")\n",
     "        return None\n",
     "    \n",
     "    # Create and save model card\n",
     "    model_card = create_tts_model_card(repo_name, MODEL_CARD_LANGUAGES)\n",
     "    readme_path = upload_path / \"README.md\"\n",
     "    with open(readme_path, \"w\") as f:\n",
     "        f.write(model_card)\n",
     "    print(\"Model card created\")\n",
     "    \n",
     "    # Upload folder\n",
     "    api = HfApi()\n",
     "    api.upload_folder(\n",
     "        folder_path=str(upload_path),\n",
     "        repo_id=repo_id,\n",
     "        repo_type=\"model\",\n",
     "    )\n",
     "    \n",
     "    print(f\"\\nUpload complete!\")\n",
     "    print(f\"View model at: https://huggingface.co/{repo_id}\")\n",
     "    return repo_id\n",
     "\n",
     "\n",
     "# Uncomment to upload\n",
     "# upload_tts_model(TTS_MODEL_PATH, TTS_REPO_NAME, HF_USERNAME)\n",
     "\n",
     "print(\"TTS upload function ready.\")\n",
     "print(\"Run: upload_tts_model(TTS_MODEL_PATH, TTS_REPO_NAME, HF_USERNAME)\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7.7 Upload Both Models"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "def upload_all_models(username: str, private: bool = False):\n",
     "    \"\"\"Upload both ASR and TTS models to HuggingFace.\"\"\"\n",
     "    print(\"=\" * 60)\n",
     "    print(\"UPLOADING ALL MODELS TO HUGGINGFACE\")\n",
     "    print(\"=\" * 60)\n",
     "    \n",
     "    results = {}\n",
     "    \n",
     "    # Upload ASR\n",
     "    print(\"\\n[1/2] Uploading ASR Model...\")\n",
     "    asr_result = upload_asr_model(ASR_MODEL_PATH, ASR_REPO_NAME, username, private)\n",
     "    results['asr'] = asr_result\n",
     "    \n",
     "    # Upload TTS\n",
     "    print(\"\\n[2/2] Uploading TTS Model...\")\n",
     "    tts_result = upload_tts_model(TTS_MODEL_PATH, TTS_REPO_NAME, username, private)\n",
     "    results['tts'] = tts_result\n",
     "    \n",
     "    # Summary\n",
     "    print(\"\\n\" + \"=\" * 60)\n",
     "    print(\"UPLOAD SUMMARY\")\n",
     "    print(\"=\" * 60)\n",
     "    for model, result in results.items():\n",
     "        status = \"SUCCESS\" if result else \"FAILED\"\n",
     "        print(f\"  {model.upper()}: {status}\")\n",
     "        if result:\n",
     "            print(f\"    URL: https://huggingface.co/{result}\")\n",
     "    \n",
     "    return results\n",
     "\n",
     "\n",
     "# Uncomment to upload both models\n",
     "# upload_all_models(HF_USERNAME, private=False)\n",
     "\n",
     "print(\"Ready to upload! Steps:\")\n",
     "print(\"  1. Set HF_USERNAME to your HuggingFace username\")\n",
     "print(\"  2. Run: hf_login() to authenticate\")\n",
     "print(\"  3. Run: upload_all_models(HF_USERNAME)\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "## 7.8 Push Model with Git LFS (Alternative Method)"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
     "# Alternative: Use Git LFS for large files\n",
     "# This is useful if you have very large model files\n",
     "\n",
     "print(\"\"\"\n",
     "ALTERNATIVE: Git LFS Upload\n",
     "===========================\n",
     "\n",
     "For very large models, you can use Git LFS:\n",
     "\n",
     "1. Install Git LFS:\n",
     "   git lfs install\n",
     "\n",
     "2. Clone your HuggingFace repo:\n",
     "   git clone https://huggingface.co/YOUR_USERNAME/YOUR_REPO\n",
     "   cd YOUR_REPO\n",
     "\n",
     "3. Copy model files:\n",
     "   cp -r /path/to/model/* .\n",
     "\n",
     "4. Track large files with LFS:\n",
     "   git lfs track \"*.bin\"\n",
     "   git lfs track \"*.safetensors\"\n",
     "   git lfs track \"*.pth\"\n",
     "\n",
     "5. Commit and push:\n",
     "   git add .\n",
     "   git commit -m \"Add model files\"\n",
     "   git push\n",
     "\n",
     "This method is recommended for files > 5GB.\n",
     "\"\"\")"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
     "---\\n",
     "# Appendix\\n",
     "---"
    ]
   },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1: Troubleshooting Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "TROUBLESHOOTING GUIDE\n",
    "====================\n",
    "\n",
    "1. OUT OF MEMORY (OOM) ERROR\n",
    "   - Reduce batch_size (try 1 or 2)\n",
    "   - Enable gradient_checkpointing\n",
    "   - Use 8-bit quantization\n",
    "   - Clear cache: torch.cuda.empty_cache()\n",
    "\n",
    "2. SLOW TRAINING\n",
    "   - Enable fp16 (mixed precision)\n",
    "   - Increase num_workers for data loading\n",
    "   - Use gradient accumulation instead of larger batch\n",
    "\n",
    "3. ROBOTIC TTS OUTPUT\n",
    "   - Train for more epochs\n",
    "   - Use cleaner audio data\n",
    "   - Try different speaker reference\n",
    "\n",
    "4. POOR ASR ACCURACY\n",
    "   - Add more training data\n",
    "   - Fine-tune longer\n",
    "   - Mix clean and noisy data\n",
    "\n",
    "5. WRONG PRONUNCIATION\n",
    "   - Phonetize text input\n",
    "   - Add more examples in training data\n",
    "   - Lower learning rate\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2: Language Code Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "LANGUAGE CODE REFERENCE\n",
    "=======================\n",
    "\n",
    "Language        MMS (ASR)    NLLB (Translation)    TTS Speaker\n",
    "-----------    ---------    ------------------    -----------\n",
    "Akan (Twi)      aka          aka_Latn              Twi_Speaker\n",
    "Ewe             ewe          ewe_Latn              Ewe_Speaker\n",
    "Ga              gaa          gaa_Latn              Ga_Speaker\n",
    "Dagbani         dag          dag_Latn              Dagbani_Speaker\n",
    "Dagaare         dga          N/A                   N/A\n",
    "English         eng          eng_Latn              N/A\n",
    "Hausa           hau          hau_Latn              N/A\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3: Model Card & Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "MODEL CITATIONS\n",
    "===============\n",
    "\n",
    "MMS (Massively Multilingual Speech):\n",
    "  @article{pratap2023mms,\n",
    "    title={Scaling Speech Technology to 1,000+ Languages},\n",
    "    author={Pratap, Vineel and others},\n",
    "    journal={arXiv preprint arXiv:2305.13516},\n",
    "    year={2023}\n",
    "  }\n",
    "\n",
    "NLLB-200:\n",
    "  @article{costa2022no,\n",
    "    title={No Language Left Behind},\n",
    "    author={Costa-juss\\`a, Marta R and others},\n",
    "    journal={arXiv preprint arXiv:2207.04672},\n",
    "    year={2022}\n",
    "  }\n",
    "\n",
    "XTTS:\n",
    "  @misc{coqui-ai-tts,\n",
    "    author={Coqui AI},\n",
    "    title={XTTS: Cross-Lingual Text-to-Speech},\n",
    "    year={2023},\n",
    "    url={https://github.com/coqui-ai/TTS}\n",
    "  }\n",
    "\n",
    "UGSpeechData:\n",
    "  @article{ugspeechdata2024,\n",
    "    title={UGSpeechData: A Multilingual Speech Dataset of Ghanaian Languages},\n",
    "    author={University of Ghana HCI Lab},\n",
    "    year={2024}\n",
    "  }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A4: Next Steps & Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "FUTURE IMPROVEMENTS\n",
    "===================\n",
    "\n",
    "1. ADD MORE LANGUAGES\n",
    "   - Dagaare, Ikposo from UGSpeechData\n",
    "   - Nzema, Dangme from other sources\n",
    "\n",
    "2. IMPROVE TTS QUALITY\n",
    "   - Collect more studio-quality recordings\n",
    "   - Train language-specific vocoders\n",
    "   - Add emotion/prosody control\n",
    "\n",
    "3. REDUCE LATENCY\n",
    "   - Implement streaming ASR\n",
    "   - Use smaller distilled models\n",
    "   - Optimize with TensorRT/ONNX\n",
    "\n",
    "4. EXPAND DOMAINS\n",
    "   - Healthcare terminology\n",
    "   - Agricultural content\n",
    "   - Educational materials\n",
    "\n",
    "5. MOBILE DEPLOYMENT\n",
    "   - Quantize models for mobile\n",
    "   - Create Android/iOS SDK\n",
    "   - Offline capability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook\n",
    "\n",
    "**Congratulations!** You've completed the Ghana Speech-to-Speech Pipeline tutorial.\n",
    "\n",
    "For questions or contributions, please open an issue on the project repository.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
